use std::fmt;

#[derive(Debug, PartialEq, Copy, Clone)]
/// Represents a type of a Modelica token. Defined based on [Modelica
/// Specification
/// 3.6](https://specification.modelica.org/maint/3.6/modelica-concrete-syntax.html).
pub enum TokenKind {
    // Custom token type indicating the end of file
    EOF,
    // Custom token type indicating lexical errors
    Error,
    Comma,
    Dot,
    Semi,
    Colon,
    LParen,
    RParen,
    LCurly,
    RCurly,
    LBracket,
    RBracket,
    Equal,
    Assign,
    Plus,
    Minus,
    Star,
    Slash,
    Flex,
    DotPlus,
    DotMinus,
    DotStar,
    DotSlash,
    DotFlex,
    Gre,
    Geq,
    Les,
    Leq,
    Neq,
    Eq,
    Not,
    And,
    Or,
    In,
    For,
    If,
    Else,
    Elif,
    Then,
    When,
    Elwhen,
    While,
    Loop,
    Break,
    Return,
    Partial,
    Class,
    Operator,
    Expandable,
    Model,
    Function,
    Record,
    Type,
    Block,
    Connector,
    Package,
    Pure,
    Impure,
    Initial,
    Equation,
    Algorithm,
    Extends,
    Import,
    Public,
    Protected,
    Within,
    Final,
    Encapsulated,
    Enumeration,
    Input,
    Output,
    Redeclare,
    Inner,
    Outer,
    Replaceable,
    Constrainedby,
    Flow,
    Stream,
    Discrete,
    Parameter,
    Constant,
    Each,
    Annotation,
    External,
    End,
    Der,
    Connect,
    LineComment,
    BlockComment,
    Ident,
    String,
    Uint,
    Ureal,
    True,
    False,
}

#[derive(Debug, PartialEq, Copy, Clone)]
/// Used to represent token's position in the input string
///
/// * `pos`: index of the character that corresponds with this position
/// * `line`: line number of the character that corresponds with this
///   position
/// * `col`: column number of the character that corresponds with this
///   position
pub struct Position {
    pub pos: usize,
    pub line: usize,
    pub col: usize,
}

#[derive(Debug, PartialEq, Clone)]
/// Represents a single Modelica token.
///
/// Tokens contain information on their type and their coordinates in
/// the source.
///
/// * `idx`: position in the token collection
/// * `text`: text content of the token
/// * `typ`: token's type
/// * `start`: position of the first character
/// * `end`: position of the last character
pub struct Token {
    /// Index of the token in the input
    pub idx: usize,
    /// Text of the token
    pub text: String,
    /// Token's type
    pub typ: TokenKind,
    /// Position of staring character in the input
    pub start: Position,
    /// Positon of ending character in the input
    pub end: Position,
}

impl fmt::Display for Token {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "{:?}({})", self.typ, self.text)
    }
}

/// Represents collection of Modelica tokens generated by the lexer.
///
/// This object is used in the parsing process. Afterward it should be
/// used together with the list of syntax events to produce parse trees
/// or ASTs.
pub struct TokenCollection {
    all: Vec<Token>,
    tokens: Vec<usize>,
    comments: Vec<usize>,
    errors: Vec<usize>,
}

impl Default for TokenCollection {
    fn default() -> Self {
        Self::new()
    }
}

impl TokenCollection {
    /// Return a new empty token collection. Should be called by the lexer.
    pub fn new() -> Self {
        TokenCollection {
            all: Vec::new(),
            tokens: Vec::new(),
            comments: Vec::new(),
            errors: Vec::new(),
        }
    }

    /// Push a new token to the collection.
    ///
    /// This function should be called by the lexer.
    ///
    /// * `text`: token's text content
    /// * `typ`: token's type
    /// * `start`: position of staring character in the input
    /// * `end`: Position of ending character in the input
    pub(crate) fn push(&mut self, text: String, typ: TokenKind, start: Position, end: Position) {
        let token = Token {
            idx: self.all.len(),
            text,
            typ,
            start,
            end,
        };
        match typ {
            TokenKind::BlockComment | TokenKind::LineComment => self.comments.push(token.idx),
            TokenKind::Error => self.errors.push(token.idx),
            _ => self.tokens.push(token.idx),
        }
        self.all.push(token);
    }

    /// Return number of all lexical elements in the collection.
    ///
    /// This function doesn't discriminate between semantically
    /// meaningful tokens, comments and lexical errors.
    pub fn len(&self) -> usize {
        self.all.len()
    }

    /// Return number of semantically meaningful tokens, ie. not comments and not errors.
    pub fn token_count(&self) -> usize {
        self.tokens.len()
    }

    /// Return number of comments.
    pub fn comment_count(&self) -> usize {
        self.tokens.len()
    }

    /// Return number of lexical errors.
    pub fn error_count(&self) -> usize {
        self.errors.len()
    }

    /// Return i-th item (token, comment or error). Return `None` if
    /// there is no such item.
    pub fn get_item(&self, i: usize) -> Option<&Token> {
        self.all.get(i)
    }

    /// Return i-th token. Return `None` if there is no such token.
    pub fn get_token(&self, i: usize) -> Option<&Token> {
        if let Some(j) = self.tokens.get(i) {
            self.all.get(*j)
        } else {
            None
        }
    }

    /// Return i-th comment. Return `None` if there is no such comment.
    pub fn get_comment(&self, i: usize) -> Option<&Token> {
        if let Some(j) = self.comments.get(i) {
            self.all.get(*j)
        } else {
            None
        }
    }

    /// Return i-th error. Return `None` if there is no such error.
    pub fn get_error(&self, i: usize) -> Option<&Token> {
        if let Some(j) = self.errors.get(i) {
            self.all.get(*j)
        } else {
            None
        }
    }

    /// Return iterator over lexical errors
    pub fn errors(&self) -> impl Iterator<Item = &Token> {
        self.errors
            .iter()
            .map(|i| self.all.get(*i).unwrap())
    }
}
